---

title:                 "SeqWare Pipeline: Basic Deciders"
markdown:              advanced
toc_includes_sections: true
is_dynamic:            true

---


## Overview

So far you've seen how to write a module to wrap a given tool in a consistent way. You've also seen how to write a workflow that links together multiple modules and parameterizes them through a simple ini file. Finally, you can put these together and run the workflow using the commands you saw in the introduction with HelloWorld. But the reality of large projects dictates that you can't run each workflow manually. That's where deciders come in, they essentially are a bit of code that links up the metadata in the MetaDB with a given workflow based on a set of rules encoded in the decider. At UNC we have deciders for each of our workflows and they let us process all the TCGA data on hourly cron jobs. These deciders, for example, look in the database for all RNASeq human samples that have not previously been processed through a workflows, pulls back the needed data from the MetaDB, and launches a workflow run for that particular lane. This is the heart of what a decider is trying to do, it links metadata to the actual execution of workflows in an automated way.

So we created the deciders for:

* automation, running workflows as a cron
* a place for nasty code, frequently changing code, other code that is site specific 

For the last point we've tried to make modules very clean and generic with little business logic, mainly focused on parameterization for behavior. The workflows are very dumb as well and really depend on parameterization. So we've essentially used the deciders as places to contain logic that might be site specific or simply frequently changing. 

## API Classes

The interface for a decider is [DeciderInterface](http://seqware.github.com/javadoc/git_0.13.4/apidocs/net/sourceforge/seqware/pipeline/decider/DeciderInterface.html). In practice, when creating your own deciders you will probably extend [BasicDecider](http://seqware.github.com/javadoc/git_0.13.4/apidocs/net/sourceforge/seqware/pipeline/deciders/BasicDecider.html)

## BasicDecider

Full [Command-line options for the Basic Decider](/docs/17-plugins/#basicdecider) are available. Below, we will high-light some key points. 

Required parameters of note:

* wf-accession: The SWID for the workflow that you wish to run
* study-name OR sample-name OR all OR sequencer-run-name: Limit the subset of samples to look for files associated with particular study, sample, sequencer-run, or just run over everything.
* parent-wf-accessions OR meta-types: Limit the files under consideration to those of specific file types (such as (gz) files or those generated by particular parent workflows. 

Optional parameters of note:

* check-wf-accessions: Comma-separated accessions of workflows that you may already be satisfied with (for example, satisfactory runs of an older version of a workflow). If all files in a particular group are "contained" within one of the workflow runs in this list, it will not be re-run.  
* rerun-max: The maximum number of times to re-launch a workflowrun if failed. Default: 5.
* group-by: Group by one of the headings in FindAllTheFiles. Default: FILE_SWA.

When extending the BasicDecider, you may also place the parent-wf-accessions, check-wf-accessions, and wf-accession into a decider.properties file to permanently set these values. 

## Decision Logic

The BasicDecider performs the following steps:

1. In the init method
	* Must have one and only one of study-name, sample-name, sequencer-run-name or all
	* Pulls the parent-workflow-accessions, check-wf-accessions, and workflow-accession out of the decider.properties file (but doesn't fail if the file doesn't exist)
	* From the command line:
		* Determines the grouping strategy. This must be one of the header values from the SymLinkFileReporter
		* Overrides the workflow-accession, parent-workflow-accessions and check-wf-accessions if they exist.
		* Sets the metatypes (0 or more)
		* Sets force-run-all
		* Sets Test
			* if test is enabled, print the symlinkreporter header
		* Sets whether or not to use the skip flag (by default skip is respected)
		* Sets whether or not to use metadata writeback
		* Sets whether to immediately run or to schedule the workflows
		* Sets the maximum number of launches
		* Sets the maximum number of re-runs of failed workflows
	* Checks to see if one of parent-workflow-accession or meta-type is set, fails otherwise
2. In the do_run method
	* If run type is all, iterate through all studies
		* find all the files for a certain study
		* [Separate files](#Separate files) based on grouping
		* [Launch workflows](#Launch workflows) on the separated files
		* return
	* If the run type is study-name
		* find all the files for the study
	* If the run type is sample-name
		* find all files for the sample
	* If the run type is sequencer-run-name
		* find all the files for the sequencer run
	* [Separate files](#Separate files) based on the grouping strategy
	* [Launch workflows](#Launch workflows) on the separated files

### Separate files

* iterate through all of the files given
* edit the group-by attribute if necessary using the handleGroupByAttribute method (extend for your decider)
* put into a hash based on the modified group-by attribute

### Launch workflows

1. For each grouping, iterate through the files
	* save the workflow-accession of the file
	* check this workflow-accession to see if it is an equivalent run, e.g. the same workflow accession or on the list of check-workflow-accessions. If so, save to a var if no parent accessions were given or if the parent accession is in the list
		* check for each file if the metatype is correct (if it exists), then [Add file to set](#add_file_to_set)
		* or just [Add file to set](#add_file_to_set) to be run
2. If the parent accessions (e.g. Processing event producing input file), files to run and workflow parent accessions to run (e.g. IUS) are not empty
	* check whether or not to [Rerun workflow run](#rerun_workflow_run)
	* create the ini file (extend for your decider)
	* do the final check on your set of files (extend for your decider)
	* if force-run-all is enabled, negate any earlier flags
	* if we're in testing mode or we don't want to rerun
		* print the command line
	* otherwise if the max launched is less than the amount given
		* construct a Workflow object and either launch it or schedule it
	* otherwise
		* Print an error message and exit

### Add file to set

* Check the file details (extend for your decider, should always 'super' on this method)
	* checks to see whether the file row contains any trace of the word 'skip', and removes it.
* If running in test mode, print the file metadata
* Add to list of files to run and parent accessions
* Add the IUS to the list of parent-accessions-to-run

### Rerun workflow run

* [Search for Relevant Predecessors](#Search for Relevant Predecessors) using search type 1 
	* Iterate through previous workflow runs
		* For each previous run , get the workflow run report and consult the [Decision Table](#Decision Table) in order to determine which course of action to take. 
	* Iterate through previous workflow runs to check
 		* Get the workflow run report and if all of the files to run were run using the previous run to check, then we also block a rerun. 
* If the rerun has not been set to false, repeat using using search type 2 
* If the rerun has not been set to false, repeat using using search type 3 

### Decision Table

A general guide-line here is that we only count failures when they are associated with workflow runs that ran on the same files (and paths) as we are currently attempting to run on. 

| Status of previous workflow run | 1 (More files to run than were ran in the past  | 2 (Same files on different paths) | 3 (Same files on same paths) | 4 (More files were ran in the past |
|--------------------|--------------|---------------|----------|--------|
|Failed| rerun | rerun | count as failure, rerun | rerun (with warning message)|
|Other (pending, running, no status, etc.) | do not rerun | do not rerun | do not rerun | do not rerun |
|Completed| rerun | rerun | do not rerun | do not rerun |

In addition, if for a group of files, we also count up more failures than rerun-max, then we also block rerun. 

### Search for Relevant Predecessors

When looking for previous workflow runs run on these files, we will search under three search patterns.

1. Look for Workflow Runs which are linked to a File via the File, processing_file, Processing, processing_relationship, Processing, Workflow_Run path. i.e. Are there completed workflow runs (whether previous runs or previous workflows to check) that disallow running on this file?  
2. Look for Workflow Runs which are linked to a File via the File, processing_file, Processing, Workflow_Run, IUS, Workflow_Run path. In addition, for each of these resulting Workflow Runs, we ensure that they are not connected to Processing. i.e. Are there workflow runs in progress that share the same IUS as one of our files (and thus are not hooked up to the processing hierarchy)? 
3. The same as 2. except with Lane. i.e. Are there workflow runs in progress that share the same Lane as one of our files (and thus are not hooked up to the processing hierarchy)? 


<!-- when we have more time, we need to figure out an automatic way of syncing this with https://wiki.oicr.on.ca/display/PIPEDEVAL/Writing+a+decider -->


