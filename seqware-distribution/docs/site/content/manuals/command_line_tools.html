--- 
title: Command-Line Tools
---


Command-Line Tools
====================


About
----------------

This guide will show you how to use command line tools to access the SeqWare metadatabase. This will allow you to do the following tasks using tools that can be scripted versus our Portal web-based interface that requires a user to click on an interface:

* upload data such as fastq files to the cloud and associate that data with particular samples
* find the list of available workflows and the parameters they accept
* schedule a workflow and monitor its progress
* find a list of files produced by a workflow
* download files produced by a workflow

The command line tools are Java wrappers for our RESTful Web Service. If you would like to learn more about the low-level API (perhaps you want to call it directly in a program or script) you can find more information on our old SeqWare wiki page: [SeqWare WebService](https://sourceforge.net/apps/mediawiki/seqware/index.php?title=SeqWare_WebService).

Tools and Requirements
--------------------------------

You will need to download the SeqWare jar file from our continuous build server. Please choose the jar that has the -full suffix, e.g. [seqware-distribution-0.12.5-SNAPSHOT-full.jar](http://jenkins.res.oicr.on.ca/job/seqware/lastStableBuild/net.sourceforge.seqware$seqware-distribution/).

SeqWare is open source architecture built in Java. The jar file contains client code that will allow you to interact with a web service (either on a VM, installed on a cluster or in the cloud) that controls, among other things, workflow execution. You should use this tool on a recent Linux distribution (like RedHat, Ubuntu, or CentOS) using Oracle Java 1.6 or newer.  MacOSX and Windows have not been tested.

Once you download the jar file you will need to setup a configuration file. By default the location is ~/.seqware/settings.

This file contains the web address of the RESTful web service, your username and password, and you Amazon public and private keys that will allow you to push and pull data files to and from the cloud.  Here is an example settings file, please check with the SeqWare Help Desk to ensure your settings are correct, especially the SW_REST_URL since that might change:

	# GENERAL
	SW_METADATA_METHOD=webservice
	# API
	# the base URL for the RESTful SeqWare API
	# this is our production server’s address
	SW_REST_URL=http://SeqWareWS
	# the username and password to connect to the REST API, this is used by SeqWare Pipeline to write back processing info to the DB
	SW_REST_USER=user@host.com
	SW_REST_PASS=password
	# Amazon Cloud Settings
	# used by tools reading and writing to S3 buckets (dependency
	# data/software bundles, inputs, outputs, etc)
	AWS_ACCESS_KEY=example
	AWS_SECRET_KEY=example

Creating Studies, Experiments, and Samples
----------------------------------------------

You can do this either with the [Portal](http://SeqWarePortal "SeqWare Portal") or via the command line tools below.

You will want to set up your study, experiments, and samples before uploading your fastq or other data files.  This ensures you have “parents” to attach these files to.  Otherwise you will not be able to use them as parameters for workflows. In the following screenshot you see the links that allow you to create these entities through the web application.

There are also command line tools for creating study, experiment, and samples.  The functionality for each of these is fairly limited.  For example, the rich descriptive language for how a read is structured is not yet fully supported and updates to existing entities are not yet possible.  However the basic functionality to create studies, experiments, and samples now exists in a scriptable form.

First, you can find out what tables this tool is capable of writing:

	java -jar seqware-pipeline-0.11.4-r4770.jar -p net.sourceforge.seqware.pipeline.plugins.Metadata -- --list-tables
	
	TableName
	study
	experiment
	sample

Now, for a given table, you can find out what fields you can write back to and their type:

	java -jar seqware-pipeline-0.11.4-r4770.jar -p net.sourceforge.seqware.pipeline.plugins.Metadata -- --table study --list-fields
	
	Field    Type    Possible_Values
	title    String
	description    String
	accession    String
	center_name    String
	center_project_name    String
	study_type    Integer    [1: Whole Genome Sequencing, 2: Metagenomics, 3: Transcriptome Analysis, 4: Resequencing, 5: Epigenetics, 6: Synthetic Genomics, 7: Forensic or Paleo-genomics, 8: Gene Regulation Study, 9: Cancer Genomics, 10: Population Genomics, 11: Other]

So using the information above I can create a new study:

	java -jar seqware-pipeline-0.11.4-r4770.jar -p net.sourceforge.seqware.pipeline.plugins.Metadata -- --table study --create --field 'title::New Test Study' --field 'description::This is a test description' --field 'accession::InternalID123' --field 'center_name::SeqWare' --field 'center_project_name::SeqWare Test Project' --field study_type::4
	
	SWID: 29830

The output of this command above includes the line “SWID: 29830”.  This is very important since this number is a unique identifier across the database and used to link together entities.  For example, you will use the number produced by the study add command as the parent for the experiment you create below.  If you do not track and supply these numbers then the hierarchy of study/experiment/sample cannot be created.

The next step is to create an experiment and link it to the study you created above. You can find the platform ID using the --list-fields option shown above:

	java -jar seqware-pipeline-0.11.4-r4770.jar -p net.sourceforge.seqware.pipeline.plugins.Metadata -- --table experiment --create --field 'title::New Test Experiment' --field 'description::This is a test description' --field study_accession::29830 --field platform_id::26
	
	SWID: 29831

Again, you use the SWID from the above output in the next step to create an associated sample. You can find the platform ID using the --list-fields option shown above:

	java -jar seqware-pipeline-0.11.4-r4770.jar -p net.sourceforge.seqware.pipeline.plugins.Metadata -- --table sample --create --field 'title::New Test Sample' --field 'description::This is a test description' --field experiment_accession::29831 --field organism_id::26
	
	SWID: 29832

At this point you should have a nice study/experiment/sample hierarchy.  You can, of course, add multiple samples per experiment and multiple experiments per study.  For each of the samples you can now upload one or more fastq files.  You will need the SWID from the sample creation above for this step.  Here is a screenshot of what the above commands produce in the Portal:


Uploading Fastq and Associating with a Sample
-------------------------------------------------

The first step in uploading a fastq file(s) and associating with a sample is to identify the sample’s SWID. The easiest way to do this is to use the Portal web application to navigate through the Study/Experiment/Sample tree to the sample you want to upload fastq for and to note its associated SWID.  You then need to know the destination “bucket” you have been assigned. If you do not know your bucket you can email the SeqWare HelpDesk at [seqware.jira@oicr.on.ca](seqware.jira@oicr.on.ca). Once you have these two pieces of information you can then use the ProvisionFiles utility to push your files into the cloud and associate them with this sample.  Here is an example of calling this utility:

	java -jar seqware-pipeline-0.11.4-r4770.jar -p net.sourceforge.seqware.pipeline.plugins.ModuleRunner -- --module net.sourceforge.seqware.pipeline.modules.utilities.ProvisionFiles --metadata-output-file-prefix s3://bucket/samplename/ --metadata-parent-accession 29832 --metadata-processing-accession-file new_accession.txt -- -im fastq::chemical/seq-na-fastq-gzip::test_R1.fastq.gz -im fastq::chemical/seq-na-fastq-gzip::test_R2.fastq.gz -o s3://bucket/samplename/

> **TIP:** you can find a list of the meta types (like chemical/seq-na-fastq-gzip above) at [Module Conventions - Module MIME Types](http://sourceforge.net/apps/mediawiki/seqware/index.php?title=Module_Conventions#Module_MIME_Types). This is the list we add to as needed when creating new workflows.  It is extremely important to be consistent with these since a workflow will not recognize your input unless the meta type string matches what it expects exactly.

In this example it will upload two fastq files to s3://bucket/samplename and will link them to the sample identified by 29832 (the sample’s SWID).  Use the portal to find out the SWID for an existing sample or get the SWID using the command line tool when you create a new study. Providing that here will cause the fastq to be associated with that parent sample.


Associating Existing Files with a Sample 
--------------------------------------------------
The best way to get data into the cloud is to use the ProvisionFiles utility above since it both uploads the data (using multiple threads to maximize performance) and also saves the metadata back to the database.  However, sometimes you have already uploaded data to your designated bucket and just want to link the files to particular samples or IUSs in the database.  GenericMetadataSaver is the tool you can use to accomplish this, for example:

	java -jar seqware-pipeline-0.11.4-r4770.jar -p net.sourceforge.seqware.pipeline.plugins.ModuleRunner -- --module net.sourceforge.seqware.pipeline.modules.GenericMetadataSaver --metadata-parent-accession 25192 -- --gms-output-file fastq::chemical/seq-na-fastq-gzip::s3://bucket/sample/Sample/GAA_L001_R1_001.fastq.gz --gms-algorithm UploadLane1 --gms-suppress-output-file-check

Here files are associated with the parent (SWID: 25192 which is an IUS).  These paths already exist in S3.  See List Files in S3 below for how to use a tool to list out your successfully uploaded files.  That tool gives you the s3:// URLs like the ones above.

Listing Available Workflows and Their Parameters
--------------------------------------------------------

Once you have uploaded data the next step is to find the available workflows and their parameters.  To see the list of available workflows you can execute the following command:

	java -jar seqware-pipeline-0.11.4-r4770.jar -p net.sourceforge.seqware.pipeline.plugins.BundleManager -- --list-install

You will get a tab-delimited list of workflows showing their name, version, and (most importantly) their SWID.  

> **TIP:** it may be easier for you to read the output by cutting and pasting into a spreadsheet.

In this example we are going to use the latest (at the time of this writing) comprehensive exome workflow (13224).  The output of the above command includes the line:

	ComprehensiveExomeGenomeAnalysis    0.10.5    Fri Mar 16 21:26:03 EDT 2012    13224    null/Pipeline_Bundle_ComprehensiveExomeGenomeAnalysis_0.10.5_SeqWare_0.10.0

The fourth column includes the SWID for this workflow that you will use in the next command to find all the parameters (and their defaults) that this workflow takes.  Here is the command, notice I redirect the output to create a basic ini file that can later be customized and used to submit a run of this workflow:

	java -jar seqware-pipeline-0.11.4-r4770.jar -p net.sourceforge.seqware.pipeline.plugins.BundleManager -- --list-workflow-params --workflow-accession 27133 > workflow.ini

In this example the workflow “ComprehensiveExomeGenomeAnalysis” version 0.10.5 (SWID 13224) parameters are listed.  The output conforms to the input you can use to parameterize and launch workflows.  For example:

	#key=min_freq:type=text:display=T:display_name= Min Allele Frequency
	min_freq=0.2
	#key=novoalign_gap_open_param:type=text:display=T:display_name=Novoalign Gap Open Penalty (-g, default 40)
	novoalign_gap_open_param=40

> **NOTE:** the lines above have been wrapped but you should not include line breaks in your file.  Instead, make sure the file that you create using this tool (and customize for later launching a workflow) includes comment lines starting with “#” and the key=value lines only.  In the command above the redirect to the file workflow.ini will include some extra lines of status output.  Make sure you remove these before continuing to launch the workflow with this ini file.  You can customize any values from the key/value pairs that you need to.  For example, the most frequent parameters you will customize are input files.  In the workflow example above you will want to customize input_fastq_1 and input_fastq_2.  For example, if you wanted to process the files you uploaded you would customize these lines as:

	input_fastq_1=s3://bucket/samplename/test_R1.fastq.gz
	input_fastq_2=s3://bucket/samplename/test_R2.fastq.gz

Since this is a low-level tool you may see many more parameters exposed with this tool than you would using the web Portal application.  Please use caution when customizing these values since some refer to items that affect the underlying infrastructure on EC2. If anything is not clear please consult with the Help Desk (seqware.jira@oicr.on.ca) so we can work with you to prepare your workflow configuration files.

> **TIP:** when you customize key-values in the ini file prepared above you do not need to include key-values that you leave unchanged.  If you do not include these the workflow will run with those values by default anyway.  Removing unchanged key-values will greatly reduce the size of your ini files making it much easier to see the key-values you are interested in.


Triggering a Workflow and Monitoring Progress 
------------------------------------------------------

At this point you know what workflow you are going to run and you have a customized ini file that contains, for example, the input files. The next step is to trigger the workflow using the ini file you prepared.

	java -jar seqware-pipeline-0.11.4-r4770.jar -p net.sourceforge.seqware.pipeline.plugins.WorkflowLauncher -- --ini-files temp.ini --workflow-accession 13224 --schedule --parent-accessions 24656

> **TIP:** the parent-accessions is the SWID of the ProvisionFiles element that was added under the sample when use used this tool to upload the fastq files in the example above.  You MUST specify this otherwise the workflow’s results will not be linked to anything (they will be orphaned and will not be visible in the Portal or present in the reports below). Conveniently the ProvisionFiles tool will write these accessions to a file.

This schedules the workflow to run on the cloud. It can take several minutes for a node to finish launching and pick up the workflow to run.  This process can be delayed if the total number of workflows submitted exceeds the maximum simultaneous allowed workflows.  This setting allows us to control the maximum number of cluster nodes launched at any one time. It is entirely configurable, please email the Help Desk (seqware.jira@oicr.on.ca) if you want to increase the maximum number of workflows. Any workflows submitted beyond this max will simply be held in “submitted” state until other workflows finish.

Once submitted, you can use the Portal to list the number of submitted, running, and failed workflows.  Log into the Portal and click on the “Show Analysis” link under the Analysis panel.  You can then click on the tab for “Running Analysis” to see what is submitted/running/failed.
A better way of monitoring workflows (and getting a list of the outputs) is to use the WorkflowRunReporter plugin. This will let you script the monitoring of workflow runs.

	java -jar ~/seqware-pipeline-0.12.1-r4898.jar -p net.sourceforge.seqware.pipeline.plugins.WorkflowRunReporter -- -wa 61841

In this example all the status information for workflows with workflow accession 61841 are printed out to a file in the local file system.  This includes several columns of interest including the status of the workflow, the output file types, and their locations in S3 or the file system. You can use this information to automate the checking of workflows and the retrieval of the results!

In the output from the above command you will see accessions for each workflow run. If the status is “failed” you can use resources directly on the Web Service to see what went wrong by returning the stderr and stdout from the workflow. This is how you might do that for a workflow_run with an accession of 6774:

	GET -C user@user.com:password http://seqwarews3.elasticbeanstalk.com/SeqWareWS/reports/workflowruns/6774/stderr

Keep in mind two things: 

1. this is a non-standard URL since this feature is in testing
1. this direct GET request will be replaced with a command-line tool in the near future.

Downloading Workflow Results
-------------------------------------

Once a workflow has finished running you will want to list out the associated files and download the results.  While you can use the Portal for downloading files the best way to get files in bulk is to use our reporting tool. This produces a tab-delimited file that lists all the files produced for the workflows you are interested in.  You can then use the same ProvisionFiles utility above to pull files back.  Since the report produces a simple tab-delimited file you can easily automate the downloading of results by looping over the output files and calling ProvisionFiles using a script.

	java -jar seqware-pipeline-0.11.4-r4770.jar -p net.sourceforge.seqware.pipeline.plugins.SymLinkFileReporter -- --no-links --output-filename study_report --workflow-accession 13224 --study 20120403_SEQ1

The output here is a study_report.csv file that contains a line for each file (both those uploaded and those produced by workflows).  You can also filter by file types, for example if you want to see report bundles:

	java -jar seqware-pipeline-0.11.4-r4770.jar -p net.sourceforge.seqware.pipeline.plugins.SymLinkFileReporter -- --no-links --output-filename study_report --workflow-accession 13224 --study 20120403_SEQ1 --file-type application/zip-report-bundle

Or an example filtering by sample:

	java -jar seqware-pipeline-0.11.4-r4770.jar -p net.sourceforge.seqware.pipeline.plugins.SymLinkFileReporter -- --no-links --output-filename study_report --workflow-accession 13224  --sample 20120403_SEQ1_GAG

You can use these URLs (such as s3://bucket/samplename/test_R1.fastq.gz) with ProvisionFiles to download results.  Here’s an example downloading a report bundle:

	java -jar seqware-pipeline-0.11.4-r4770.jar -p net.sourceforge.seqware.pipeline.plugins.ModuleRunner -- --module net.sourceforge.seqware.pipeline.modules.utilities.ProvisionFiles --no-metadata -- -i s3://bucket/results/seqware-0.10.0_ComprehensiveExomeGenomeAnalysis-0.10.5/59491657/GAG.fa.variant_quality.gatk.hg19.report.zip -o ./

Here the zip report bundle is downloaded to the current working directory on the computer you are working on.  In this way you can pull back the results of workflows entirely through scripts that wrap the SymLinkFileReporter and ProvisionFiles.

Also note the SymLinkFileReporter gives you SWIDs for processing events and entities such as studies, samples, and experiments.  You can use this tool to find these SWIDs that are used as “parents” for workflow runs.

In addition to the command line tools, you can also use the Portal to explore the output of workflows triggered through the command line tools.

You can find more information on this report tool on the public SeqWare wiki: [Study Reporter](http://sourceforge.net/apps/mediawiki/seqware/index.php?title=SymLink_Reporter).

> **NOTE:** in the example above I use --no-metadata with ProvisionFiles. This is to prevent the tool from writing back an event to the central database. Since you are just downloading a file (versus uploading a file) you do not really want to record that download event in the database.

Listing Your Files in S3 (C)
-----------------------------

Sometimes you may want to just list all the files uploaded (for files that are the result of processing from a workflow you might want to use the reporting tool described above since it will give you more information).  To list your files in a particular bucket:

	java -jar seqware-pipeline-0.11.4-r4770.jar -p net.sourceforge.seqware.pipeline.plugins.ModuleRunner -- --no-metadata --module net.sourceforge.seqware.pipeline.modules.utilities.S3ListFiles -- -u s3://bucket/prefix > file_list.txt

The result is a file (file_list.txt above) that contains a line per file in the bucket.  Note you can specify just a bucket or a bucket and directory (prefix). The tool searches for any items in S3 that start with that bucket and prefix.

An example output:

	LISTING BUCKET: bucketname
	
	* 20120213_SEQ1_R1_001_GAA_AGA.fastq.gz size=243.5K
	* 20120213_SEQ1_R1_001_GAA_TAT.fastq.gz size=301.6K
	* 20120213_SEQ1_R1_001_GAA_CGC.fastq.gz size=31.4M
	...
	BUCKET SIZE: 207.6G

Real SeqWare Examples
-----------------------

The section below shows real examples using actual data files, upload locations, and samples from SeqWare.  Please be careful with the information below, you do not want to accidently overwrite these files for example.

### Fastq Upload

	java -jar seqware-pipeline-0.11.4-r4770.jar -p net.sourceforge.seqware.pipeline.plugins.ModuleRunner -- --module net.sourceforge.seqware.pipeline.modules.utilities.ProvisionFiles --metadata-output-file-prefix s3://abcco.uploads/cmdlinetest/ --metadata-parent-accession 28766 --metadata-processing-accession-file new_accession.txt -- -im fastq::chemical/seq-na-fastq-gzip::sample.fastq.gz -o s3://abcco.uploads/cmdlinetest/


### Associate an Existing File with Sample

	java -jar seqware-pipeline-0.11.4-r4770.jar -p net.sourceforge.seqware.pipeline.plugins.ModuleRunner -- --module net.sourceforge.seqware.pipeline.modules.GenericMetadataSaver --metadata-parent-accession 28751 -- --gms-output-file fastq::chemical/seq-na-fastq-gzip::s3://abcco.uploads/cmdlinetest/sample.fastq.gz --gms-algorithm UploadTest2 --gms-suppress-output-file-check

### Listing Workflows and Params

	java -jar seqware-pipeline-0.11.4-r4770.jar -p net.sourceforge.seqware.pipeline.plugins.BundleManager -- --list-install

	java -jar seqware-pipeline-0.11.4-r4770.jar -p net.sourceforge.seqware.pipeline.plugins.BundleManager -- --list-workflow-params --workflow-accession 27133 > workflow.ini

### Launching Workflow
Not going to really do this...

	java -jar seqware-pipeline-0.11.4-r4770.jar -p net.sourceforge.seqware.pipeline.plugins.WorkflowLauncher -- --ini-files temp.ini --workflow-accession 13224 --schedule --parent-accessions 24656
Listing Workflow Results

	java -jar seqware-pipeline-0.11.4-r4770.jar -p net.sourceforge.seqware.pipeline.plugins.SymLinkFileReporter -- --no-links --output-filename study_report --workflow-accession 13224 --study 20120213_SEQ1

	java -jar seqware-pipeline-0.11.4-r4770.jar -p net.sourceforge.seqware.pipeline.plugins.SymLinkFileReporter -- --no-links --output-filename study_report --workflow-accession 13224 --study 20120213_SEQ1 --file-type application/zip-report-bundle

	java -jar seqware-pipeline-0.11.4-r4770.jar -p net.sourceforge.seqware.pipeline.plugins.SymLinkFileReporter -- --no-links --output-filename study_report --workflow-accession 13224 --study 20120213_SEQ1 --file-type application/zip-report-bundle --sample 20120213_MISEQ1_TCCTGAGC_TAGATCGC


### Downloading Workflow Results
	java -jar seqware-pipeline-0.11.4-r4770.jar -p net.sourceforge.seqware.pipeline.plugins.ModuleRunner -- --module net.sourceforge.seqware.pipeline.modules.utilities.ProvisionFiles --no-metadata -- -i s3://seqwareinformatics.analysis/results/seqware-0.10.0_ComprehensiveExomeGenomeAnalysis-0.10.5/69212341/0-1-20120213_SEQ1CGC.fa.alignment.report.zip -o ./

### Listing Your Files in S3

	java -jar seqware-pipeline-0.11.4-r4770.jar -p net.sourceforge.seqware.pipeline.plugins.ModuleRunner -- --no-metadata --module net.sourceforge.seqware.pipeline.modules.utilities.S3ListFiles -- -u s3://abcco.uploads > file_list.txt


To Do
------

Here are the items we are currently working on to improve the command line tools. The goal is to make the command line tools a fully functional replacement for the Portal. This will allow clients to increase the throughput of file upload and workflow triggering through the system since the tools can be easily scripted.

* command line tools to list, search, and add Study, Sample, and Experiments without having to use the Portal
* displaying SWID on every entity in the Portal (making it easier to “attach” items such as Fastq files to items you see in the Portal)
* command line tool for listing and monitoring active workflows
* command line tool  for listing the parameters (ini_file field) that were used for a given workflow run (and maybe the full DAX too)
